{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "## Paper: Measuring Semantic Similarity of Word Pairs Using Path and Information Content\n",
    "#### Authors: Lingling Meng, Runqing Huang and Junzhong Gu\n",
    "\n",
    "This is a basic implementation of above mentioned paper. Please let me know if I got something wrong in it. \n",
    "The paper is very elementary in terms of WordNet-based semantic similarity. The new measure of similarity that is proposed in the paper is a continuation of Lin-similarity measure. However it seems to perform well for certain scenarios, it does not work well globally. \n",
    "\n",
    "\n",
    "#### Dataset Format:\n",
    "If the sentence pairs are to be used from a dataset file, please make sure that the dataset is in this format (or change the code relatively) \n",
    "Format: \n",
    "- SentenceA[TAB-Key]Sentence-B[TAB-Key]Known-Similarity[NextLineCharacter]\n",
    "- SentenceA[TAB-Key]Sentence-B[TAB-Key]Known-Similarity[NextLineCharacter]\n",
    "- SentenceA[TAB-Key]Sentence-B[TAB-Key]Known-Similarity[NextLineCharacter]\n",
    "- SentenceA[TAB-Key]Sentence-B[TAB-Key]Known-Similarity[NextLineCharacter]\n",
    "- ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from nltk.corpus import wordnet as wn\n",
    "from math import  log\n",
    "from math import exp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2.2 - Point 6\n",
    "# Node Max is the number of concept in WordNet. That is to say\n",
    "# how many words are there in WordNet. For a specific version of Wordnet \n",
    "# this number is different (as more words/concepts) are added in each version. \n",
    "# For further clarification, do look at the reference [18] in paper.\n",
    "node_max = 0\n",
    "for s in wn.all_synsets():\n",
    "    node_max = node_max + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to print a few Debuggin information. \n",
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function converts word (string format) into concepts (or say SynSets in WordNet)\n",
    "# So, for example, 'dog' becomes 'dog.n.01' where it means that dog is a noun (represented by 'n')\n",
    "def create_synset_from_word(word):\n",
    "    return wn.synset(\"%s.n.01\" %(str(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2.2 - Point 1. It should be pretty clear what it means.\n",
    "def lenn(synA , synB): # 'len' is a Python function so I used lenn \n",
    "    try:\n",
    "        return synA.shortest_path_distance(synB, simulate_root=True)\n",
    "    except:\n",
    "        if DEBUG: print (\"Shortest Path Distance Error...\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2.2 - Point3. \n",
    "#Hypernyms are 'is-a' concept in WordNet. For example, car 'is a' vechicle. \n",
    "# 'person' is a 'human'.\n",
    "def depth(synA):\n",
    "    try:\n",
    "        return max([len(path) for path in synA.hypernym_paths()])\n",
    "    except:\n",
    "        if DEBUG: print (\"Synset to root depth is not found...\")\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2.2 - Point \n",
    "# The maximum depth from the certain synset (c) to its root. \n",
    "# Its like counting the distance in the graph/taxonomy.\n",
    "def deep_max(synA):\n",
    "    return synA.max_depth()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2.2 - Point 5.\n",
    "# Hyponyms are also a 'is a' concept. I don't exactly know what is the difference\n",
    "# difference between hypernyms and hyponyms, so please google that. \n",
    "def hypo(synA):\n",
    "    return (len(synA.hyponyms()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2.2 - Point 2. \n",
    "# Most specific subsumer (that connects two SynSets, i.e. c1 and c2) in the taxonomy/graph. \n",
    "# Its like the first common node between two synset nodes in the graph.\n",
    "def iso(c1, c2):  #Number 2\n",
    "    subsumers = c1.lowest_common_hypernyms(c2,  use_min_depth=True)\n",
    "    if len(subsumers) == 0:\n",
    "            return None\n",
    "    subsumer = c1 if c1 in subsumers else subsumers[0]\n",
    "    return subsumer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2.3.2 - Formula 10\n",
    "# Its the Information Content based on WordNet. It assumes that the \n",
    "# Synsets which are 'leafs' in a graph (most distal nodes) represents more important \n",
    "# meaning than the one nearer to root. So for example, 'Vehicle' represents less \n",
    "# information as compare to 'car' as car is the leaf-node (distal node).\n",
    "def ic(synA):\n",
    "    return 1 - ( log( hypo(synA) + 1 ) ) / ( log( node_max ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3. Formula 11. \n",
    "# The new measure of similarity between two words. \n",
    "# base_ is the base in the formula 11\n",
    "# power_ is the power in the formula \n",
    "# 'pow' function takes base_ and power_ and returns values as -> pow(2,3) = 2^3 = 8\n",
    "# Please note that author mentions that base_ will always be between 0 >= base_ <= 1 \n",
    "# (which means similarity will remain between 0 to 1) but it may not be always true. \n",
    "# Their assumption works most of the time but not always. It is not neccessary that 2*ic( iso(c1,c2) ) <= ic(c1) + ic(c2). \n",
    "# Why?, well because this assumption is based on the assumption from reference [18] which is that leaf-nodes in WordNet \n",
    "# has higher information content than the nodes nearer to 'root' which is not a proven concept but just an assumption. \n",
    "# that is why, whenever 'base_' > 1, I made it equal to 1.0 \n",
    "def new_similarity_measure(c1, c2, k = 0.08, ):\n",
    "    \n",
    "    c1 = create_synset_from_word(c1)\n",
    "    c2 = create_synset_from_word(c2)\n",
    "    power_ =  ( 1 - exp ( -k*lenn(c1,c2) ) ) /  exp ( -k*lenn(c1,c2) ) \n",
    "    base_ = 2*ic( iso(c1,c2) )  / ( ic(c1) + ic(c2) ) \n",
    "    if base_ > 1:\n",
    "        base_ = 1.0\n",
    "#     print base_, power_, lenn(c1,c2), iso(c1,c2), ic(iso(c1,c2)), ic(c1), ic(c2)\n",
    "    return pow(base_, power_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple function to process the data in file, and print it \n",
    "def process_and_find_sim():\n",
    "    \n",
    "    datafile = os.getcwd() +  \"\\\\result_word_pairs2.csv\"\n",
    "    data = open(datafile,\"r\").read()\n",
    "    datalines = data.split(\"\\n\")\n",
    "    print \"{:-^5}\\t{:-^15}\\t{:-^15}\\t{:-^15}\\t{:-^10}\\t{:-^15}\".format(\"#\",\"Word-A\",\"Word-B\",\"Calc. Sim\", \"Meas. Sim\",\"WUP Sim\")\n",
    "    \n",
    "    for line in datalines[1:]:\n",
    "        cols = line.split(',')\n",
    "        if len(cols) == 4:\n",
    "            word1, word2, groundTruth = cols[1:]\n",
    "            word1, word2 = word1.lower(), word2.lower()\n",
    "            syn1, syn2 = create_synset_from_word(word1), create_synset_from_word(word2)\n",
    "            print \"{:-^5}\\t{:-^15}\\t{:-^15}\\t{:-^15}\\t{:-^10}\\t{:-^15}\".format(cols[0],word1,word2,new_similarity_measure(word1,word2), float(groundTruth), syn1.wup_similarity(syn2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this function. \n",
    "# process_and_find_sim()\n",
    "\n",
    "# If you want to check some other word pairs. \n",
    "# You can do the following:\n",
    "new_measure = new_similarity_measure('horse', 'bye')\n",
    "print (new_measure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
